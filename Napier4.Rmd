---
title: "NER on archive data"
output: html_notebook
---

NER on archive data

I want to combine the power (and my knowledge of R), with the power (and machine learning capabilities of Python).


Load the set of libraries as well as some basic variables, such as the pointer to where data is stored and the key to access google maps api.

```{r}
prep_r_env <- function() {
library(tidyr)
library(xml2)
library(dplyr)
library(googleAuthR)
library(ggmap)
library(stringr)
library(spacyr)
library(editData)
spacy_initialize(condaenv = 'NER')
}
```

The data from the archives is stored as XML files. These have to be read and put into a format that will be recognised by NER analysers.

```{r}
read_xml_data <- function(xmlfile) {
  xmlpath <- "../Data/Geoparser/"
  xmlpath <- paste0(xmlpath,xmlfile,".xml")
  xml <- read_xml(xmlpath)
  children <- xml_children(xml)
  Title <- xml_text(xml_child(children, "Title"))
  Description <- xml_text(xml_child(children, "Description"))
  RefNo <- xml_text(xml_child(children, "RefNo"))
  df <- data.frame(RefNo,Title,Description) %>%
    pivot_longer(c('Title','Description'))
return(df)
}

```

groups of 4 words in each sentence are ana lysed, to cature entities up to four words long.
get only unique values to reduce the number of calls to google.

```{r}
get_locations <- function(geographies) {
rows <- nrow(geographies)-4
geographies <- arrange(geographies,doc_id,sentence_id,token_id)
#geographies_places <- data.frame(places=geographies$token[-1])
geographies_entity <- geographies$entity[-1]
geographies_entity <- append(geographies_entity,NA)
geographies_entity1 <- geographies$entity[-1:-2]
geographies_entity1 <- append(geographies_entity1,NA)
geographies_entity1 <- append(geographies_entity1,NA)
geographies_entity2 <- geographies$entity[-1:-3]
geographies_entity2 <- append(geographies_entity2,NA)
geographies_entity2 <- append(geographies_entity2,NA)
geographies_entity2 <- append(geographies_entity2,NA)
geographies_entity3 <- geographies$entity[-1:-4]
geographies_entity3 <- append(geographies_entity3,NA)
geographies_entity3 <- append(geographies_entity3,NA)
geographies_entity3 <- append(geographies_entity3,NA)
geographies_entity3 <- append(geographies_entity3,NA)




geographies_places <- geographies$token[-1]
geographies_places <- append(geographies_places,NA)
geographies_places1 <- geographies$token[-1:-2]
geographies_places1 <- append(geographies_places1,NA)
geographies_places1 <- append(geographies_places1,NA)
geographies_places2 <- geographies$token[-1:-3]
geographies_places2 <- append(geographies_places2,NA)
geographies_places2 <- append(geographies_places2,NA)
geographies_places2 <- append(geographies_places2,NA)
geographies_places3 <- geographies$token[-1:-4]
geographies_places3 <- append(geographies_places3,NA)
geographies_places3 <- append(geographies_places3,NA)
geographies_places3 <- append(geographies_places3,NA)
geographies_places3 <- append(geographies_places3,NA)

geographies_a <- cbind(geographies,
                       entity1=geographies_entity,
                       entity2=geographies_entity1,
                       entity3=geographies_entity2,
                       entity4=geographies_entity3,
                       token1=geographies_places,
                       token2=geographies_places1,
                       token3=geographies_places2,
                       token4=geographies_places3)


geographies_a <- geographies_a %>% replace(is.na(.), FALSE)  

geographies_b <- mutate(geographies_a,
                        result=ifelse(str_detect(entity,'_B') & str_detect(entity2,'_I') & str_detect(entity2,'_I') & str_detect(entity3,'_I'),
                                      paste(token,token1,token2,token3),
                               ifelse(str_detect(entity,'_B') & str_detect(entity1,'_I') & str_detect(entity2,'_I'), paste(token,token1,token2),ifelse(str_detect(entity,'_B') & str_detect(entity1,'_I'),paste(token,token1), paste(token)))))

for(test in c(1:4)){
geographies_res_before <- c(NA,geographies_b$result)
geographies_res_before <-geographies_res_before[1:length(geographies_res_before)-1]

geographies_b <- cbind(geographies_b,geographies_res_before)

geographies_b <- mutate(geographies_b,
                        result=ifelse(str_detect(entity,'_I'), 
                                      geographies_res_before,result))
geographies_b <- select(geographies_b, -geographies_res_before)

}
#geographies_b <- unique(filter(geographies_b,str_detect(entity,'GP'),
#                        str_detect(token,'GD',negate=TRUE)))
#  select(result))
  return(geographies_b)
}
```

For each identified name, request a location (i.e. latitude and longitude) from google.  the function may be called with and without the 'uk' attribute.  With the uk attribute, the response will try to prioritise UK place names.  Therefore the function is called twice, once to identify UK place names, and a second time, with the UK names filtered out, to find worldwide locations.

The resulting data is also written to the file system to minimise the number of times google needs to be interrogated (there is a potential cost if interrogated too many times)

```{r message = FALSE, warning=FALSE}
ask_google <- function(locations,dataset,uk=TRUE){
filename <- paste0(data_dir,"/",dataset,"/","asked_google.rds")
if(file.exists(filename) & uk) {
  df <- readRDS(filename)
} else {
key <- readRDS(paste0(data_dir,"google-key"))
register_google(key = key)
df <- data.frame(gram = character(),lon = double(0), lat = double(0))
counter=0
#geographies_b <- geographies()
for(gram in unique(locations$result)) {
  counter <- counter + 1
  if(uk){
    gram1 <- paste0(gram,',','UK')
  } else {
    gram1 = gram
  }
  coord <- cbind(gram,geocode(gram1))
  df <- rbind(df,coord)
  print(paste
        (counter,gram))
}


names(df) <- c('result','lon','lat')

if(uk){
  saveRDS(df,filename)
}
}

df <- left_join(locations,df)
locations <- select(df,doc_id,sentence_id,token_id,entity,result)
#  filter(entity == "GPE_B")


if(uk){
df <- mutate(df, inuk = lon > -6 & lon < 4 & lat > 50 & lat < 61) 
  
#unknowns <- filter(df,is.na(inuk))

}

df <- select(df,doc_id,token,sentence_id,token_id,result,pos,tag,entity,lon,lat,inuk)

return(df)
}
```

The data is first loaded from one of a number of datasets.  

```{r}
get_text <- function(dataset) {
dir.create(paste0(data_dir,dataset))
new_filepath <- paste0(data_dir,'/',dataset,"/",'interesting_data.rds')
text <- read_xml_data(dataset)
text <- text %>% filter(value != "")
sentences <- data.frame(sapply(text,function(z) {rbind(data.frame(sentence = character(0)),z)}))
sentences <- paste0(sentences$value)
parsedtxt <- spacy_parse(sentences,tag=TRUE) 
geographies <- filter(parsedtxt,str_detect(entity,"GPE"))
organisations <- filter(parsedtxt,str_detect(entity,"ORG"))
persons <- filter(parsedtxt,str_detect(entity,"PERSON"))
noun <- filter(parsedtxt,str_detect(pos,"NOUN"))
proper_noun <- filter(parsedtxt,str_detect(pos,"PROPN"))

#change this if you want to alter the entity output
locations <- get_locations(proper_noun)
interesting_data <- list(organisations,persons,noun,proper_noun,locations,text,parsedtxt)
names(interesting_data) <- c('organisations','persons','noun','proper_noun','locations','text','parsedtxt')
saveRDS(interesting_data,new_filepath)
return(interesting_data)
}
```






now that we have found some Geogrpaphies, we can try and get their latitiude and longitude, to confirm whether they are real places or not.

First, get UK codes


```{r}
get_google_checked_complete <- function(google_checked,parsedtxt,dataset){
filename <- paste0(data_dir,"/",dataset,"/","google_checked_complete.rds")
if(file.exists(filename)) {
google_checked_complete <- readRDS(filename)
} else {
google_checked <- filter(google_checked, str_detect(result,"GD",negate = TRUE))
#google_checked_uk <- filter(google_checked, lon> -8 & lon<2 & lat < 61 & lat > 50) 
#google_checked_non_uk <- filter(google_checked, (lon<=8 | lon>=2) & (lat >= 61 | lat <= 50))
google_checked_uk <- filter(google_checked, inuk)
google_checked_non_uk <- filter(google_checked, !inuk)

google_na <- filter(google_checked,is.na(lon) | is.na(lat))
google_checked_non_uk <- rbind(google_checked_non_uk,google_na)

#google_checked_non_uk <- unique(select(google_checked_non_uk))
#google_checked_non_uk <- unique(left_join(google_checked_non_uk,select(parsedtxt, #-doc_id,-sentence_id,-token_id)))
#google_checked_non_uk <- spacy_parse(as.character(google_checked_non_uk),tag=TRUE) %>%
#  select(-token_id)
#google_checked_non_uk <- unique(filter(google_checked_non_uk,str_detect(entity,"GPE")))
google_checked_non_uk <- unique(google_checked_non_uk)
test <- get_locations(google_checked_non_uk)

google_checked_non_uk <- ask_google(test,uk=FALSE,dataset = dataset) %>%
  filter(!is.na(lon))
  #select(doc_id,sentence_id,token_id,token,pos,tag,entity,lon,lat,inuk)

google_checked_complete <- rbind(google_checked_uk,google_checked_non_uk)

google_checked_complete <- mutate(google_checked_complete,token = str_replace(token," \'","\'"))

}
saveRDS(google_checked_complete,filename)
return(google_checked_complete)
}
```

Use the locations that have been checked by Google to replace annotations to the text

```{r}
prep_new_geographies <- function(google_checked_complete,geographies,dataset) {
filename <- paste0(data_dir,"/",dataset,"/",'new_geographies.rds.csv')
if(file.exists(filename)) {
  #new_geographies <- readRDS(filename)
  new_geographies <- read.csv(filename)
} else {
#df <- select(google_checked_complete,result) 
df <- filter(google_checked_complete,!str_detect(token,"(?!U\\.K\\.)(.+\\..\\.$)")) %>%
  mutate(token=str_replace(token," \\'","\\'"),
         placename = token) 

#df <- arrange(df,doc_id,sentence_id,token_id)


#df <- separate(df,token,c("A","B","C","D","E","F"),sep = " ") %>%
#  pivot_longer(c("A","B","C","D","E","F")) %>%
#  mutate(ENTITY = ifelse(name=="A","GPE_B","GPE_I")) %>%
#  filter(!is.na(value)) %>%
#  select(value,ENTITY,placename) %>%
#  rename(token = value)


new_geographies <- unique(left_join(df,geographies)) %>%
  filter(!is.na(entity)) %>%
  arrange(doc_id,sentence_id,token_id) %>%
  rename(ENTITY = entity)
saveRDS(new_geographies,filename)
write.csv(new_geographies,filename)
}
return(new_geographies)
}


prep_new_parsed_text <- function(new_geographies,parsedtxt,dataset) {
filename <- paste0(data_dir,"/",dataset,"/",'new_parsed_text.rds')
#if(file.exists(filename)) {
#  new_parsed_text <- readRDS(filename)
#} else {

new_geographies <- select(new_geographies,-pos,-tag,-lemma)

new_parsed_text <- left_join(parsedtxt,new_geographies,
                             by = c("doc_id", "sentence_id", "token_id", "token"))
new_parsed_text <- 
  mutate(new_parsed_text,
          tag=ifelse(nchar(tag) < 2, pos, tag),
          entity=ifelse(str_detect(ENTITY,"GPE"),ENTITY,entity),
          entity=ifelse(is.na(entity),"",entity),
          label = ifelse(entity=="","O",entity)) %>% 
   #       tagentity=ifelse(entity == "", tag, entity)) %>%
   #       entityo=ifelse(str_detect(tagentity,"GPE"),tagentity,"O")) %>%
          filter(pos != 'PUNCT') %>%
  select(-ENTITY)

saveRDS(new_parsed_text,filename)
return(new_parsed_text)
}

```

-----------------------------




```{r}
sentence_representation <- function(df,dataset) {
filename <- paste0(data_dir,"/",dataset,"/",'sentence_representation.rds')
#if(file.exists(filename)){
#  mylist <- readRDS(filename)
#} else {
df <-  select(df,doc_id,sentence_id,token,pos,tag,entity,label) %>%
  mutate(sent_id = as.numeric(paste0(str_extract(doc_id,'[0-9]+'),
                                     as.character(sentence_id)))) %>%
  select(sent_id,token,pos,entity,label)
mylist <- list()
# note this code could be optimised
for(line in unique(df$sent_id)) {
  a <- filter(df,sent_id == line)
  if(nrow(a) > 2) {
  sentence <- a$token
  pos <- a$pos
  entity <- a$entity
  label <- a$label
  #tagentity <- a$tagentity
  #entityo <- a$entityo
  newline <- list(sentence,pos,entity,label)
  mylist <- append(mylist,list(newline))
  }
}

  saveRDS(mylist,filename)
  return(mylist)
}
```


```{r}
split_data <- function(data_set,dataset,train_model) {
filename_train <- paste0(data_dir,"/",dataset,"/",'train_data.rds')
filename_test <- paste0(data_dir,"/",dataset,"/",'test_data.rds')
len <- length(data_set)
if(train_model) {
data_split <- as.integer(length(data_set)*3/4)
train_data <- data_set[1:data_split]
test_data <- data_set[data_split:len]
saveRDS(train_data,paste0(data_dir,filename_train))
#traintest <- c(train_data,test_data)
} else {
test_data <- data_set
}
#names(traintest) <- c("train","test")
saveRDS(test_data,paste0(data_dir,filename_test))
return()
}
```


-----------------------------


```{r}
sent2features <- function(sentence_and_tag) {
  
  sentence <- c(sentence_and_tag[[1]])
  sen_tags =  c(sentence_and_tag[[2]])
  features = list()
  count <- 0
  for(word in sentence) {
    wordfeats = list()
    count <- count + 1
    wordfeats <- append(wordfeats,c('word' = sentence[[count]]))
    wordfeats <- append(wordfeats,c('tag' = sen_tags[[count]]))
    #wordfeats <- append(wordfeats,c('capitalised' =
    #              str_detect(sentence[[count]],regex('^[A-Z]'))))
    
    if(count == 1) {
      wordfeats <- append(wordfeats,(c("prevWord" = "<S>")))
      wordfeats <- append(wordfeats,(c("prevSecondWord" = "<S>")))
      wordfeats <- append(wordfeats,(c("prevTag" = "<S>")))
      wordfeats <- append(wordfeats,(c("prevSecondTag" = "<S>")))
    } else if(count == 2) {
      wordfeats <- append(wordfeats,c("prevWord" = sentence[[count-1]]))
      wordfeats <- append(wordfeats,c("prevSecondWord" = "</S>"))
      wordfeats <- append(wordfeats,c("prevTag" = sen_tags[[count-1]]))
      wordfeats <- append(wordfeats,c("prevSecondTag" = "</S>"))
    } else {
    wordfeats <- append(wordfeats,c("prevWord" = sentence[[count-1]]))
    wordfeats <- append(wordfeats,c("prevSecondWord" = sentence[[count-2]]))
    wordfeats <- append(wordfeats,c("prevTag" = sen_tags[[count-1]]))
    wordfeats <- append(wordfeats,c("prevSecondTag" = sen_tags[[count-2]]))
    
    }
    
    if(count == length(sentence)) {
      wordfeats<- append(wordfeats,c("nextWord" =  "</S>"))
      wordfeats = append(wordfeats,c("nextNextWord" =  "</S>"))
      wordfeats<- append(wordfeats,c("nextTag" =  "</S>"))
      wordfeats = append(wordfeats,c("nextNextTag" =  "</S>"))
    } else if(count == length(sentence)-1) {
      wordfeats <- append(wordfeats,c("nextWord" = sentence[[count +1]]))
      wordfeats <- append(wordfeats,c("nextNextWord" ="</S>"))
      wordfeats <- append(wordfeats,c("nextTag" =sen_tags[[count +1]]))
      wordfeats <- append(wordfeats,c("nextNextTag" =  "</S>"))
    } else if(count < length(sentence)-1) {
      wordfeats = append(wordfeats,c("nextWord" = sentence[[count+1]]))
      wordfeats  = append(wordfeats,c("nextNextWord" = sentence[[count+2]]))
      wordfeats = append(wordfeats,c("nextTag" = sen_tags[[count+1]]))
      wordfeats  = append(wordfeats,c("nextNexTag" = sen_tags[[count+2]]))
    }
        features <- append(features,list(wordfeats))
        feb <- list(features)
  }
    return(feb)
}

```

```{r}

get_feats <- function(dta) {
  sent_feats = c()
  feats = c()
  labels = c()
  len <- length(dta)
  for(sent in dta) {
    if(length(sent[[1]]) > 2) {
      feats <- append(feats,sent2features(sent))
      label <- append(labels,list(sent[[3]]))
      labels <- append(labels,list(sent[[4]]))
    } else {
     feats <- append(feats,NA)
      labels <- append(labels,NA)
    }

}
  #feats <- append(feats,list(feats))
  result <- list(feats,labels)
  return(result)
}


```

```{r}
get_pos <- function(a){
  b <- lapply(a, function(x) x = x[[2]])
  return(b)
}

```



```{r}
get_full_results <- function(X_dev,Y_dev,y_pred) {

pos <- lapply(X_dev, function(x) x = get_pos(x))

full_results <- data.frame(cbind(X_dev,pos,Y_dev,y_pred))
full_results <- mutate(full_results,comp = as.character(y_pred)==as.character(Y_dev))
return(full_results)
}

get_ner_results <- function(full_results) {
predictions <- unlist(full_results$y_pred)
test <- unlist(full_results$Y_dev)
pos <- unlist(full_results$pos)
ner_results <- data.frame(cbind(pos,test,predictions))
ner_results <- mutate(ner_results,comp = as.character(test)==as.character(predictions))
word_list <- unlist(full_results$X_dev, recursive  = FALSE)
word_list <- data.frame(unlist(lapply(word_list,function(z) z[1])))
names(word_list) = c('word')
ner_results <-cbind(word_list,ner_results) %>%
  filter(pos == 'PROPN')
}

evaluate <- function(full_results) {
predictions <- unlist(full_results$y_pred)
test <- unlist(full_results$Y_dev)
ner_results <- get_ner_results(full_results)
ner_results <- filter(ner_results,str_detect(test,'GPE')|str_detect(predictions,'GPE'))
summ <- data.frame(table(full_results$comp),row.names = TRUE)
total_predictions <- sum(summ$Freq)
correct_predictions <- summ['TRUE',]
incorrect_predictions <- summ['FALSE',]
classification_accuracy <- correct_predictions* 100 / total_predictions


false_positives <- nrow(filter(ner_results, str_detect(predictions,"GPE"),comp == FALSE))

false_negatives <- nrow(filter(ner_results, str_detect(test,"GPE"), comp == FALSE))

true_positives <- nrow(filter(ner_results,str_detect(predictions, "GPE"), comp == TRUE))
true_negatives <- nrow(filter(ner_results, !str_detect(predictions,"GPE"), comp == TRUE))
predicted_positives <- nrow(filter(ner_results, str_detect(predictions, "GPE")))
actual_positives <- nrow(filter(ner_results, str_detect(test, "GPE")))

precision <- true_positives / predicted_positives
recall <- true_positives / actual_positives
FScore <- (2 * precision * recall) / (precision + recall)
results <- c(false_positives,false_negatives,true_positives,true_negatives,precision,recall,FScore)
results <- lapply(results,function(x) as.character(round(x, digits=3)))
names(results) <- c('false_positives','false_negatives','true_positives','true_negatives','precision','recall','FScore')
return(results)
}

get_confusion_matrix <- function(false_negatives, true_positives, true_negatives, false_positives) {
TClass <- factor(c(0, 0, 1, 1))
PClass <- factor(c(0, 1, 0, 1))
Y      <- c(false_negatives, true_positives, true_negatives, false_positives)
df <- data.frame(TClass, PClass, Y)
}

draw_confusion_matrix <- function(df) {
pic <- ggplot(data =  df, mapping = aes(x = TClass, y = PClass, label)) +
  geom_tile(aes(fill = c(1,2,3,4)), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
  scale_fill_gradient(low = "green1", high = "green4") +
  scale_x_discrete(breaks=c("0","1"),
        labels=c("Positive", "Negative")) +
  scale_y_discrete(breaks=c("0","1"),
        labels=c("Negative", "Positive")) +
  theme_minimal() + theme(legend.position = "none") +
  theme(axis.text.y = element_text(face="bold", color="#993333", 
                          size=14, angle=90, hjust = 1)) +
  theme(axis.text.x = element_text(face="bold", color="#993333", 
                          size=14)) +
  theme(axis.title = element_blank()) +
  theme(axis.ticks = element_blank())
  ggtitle("Confusion Matrix")
  return(pic)
}

```


```{r}
get_result_data <- function(dataset) {
  X_dev_filename <- paste0(data_dir,"/",dataset,"/",'X_dev.rds')
  Y_dev_filename <- paste0(data_dir,"/",dataset,"/",'Y_dev.rds')
  Y_pred_filename <- paste0(data_dir,"/",dataset,"/",'Y_pred.rds')
  if(file.exists(Y_pred_filename)) {
    y_pred <- readRDS(Y_pred_filename)
  } else {
    X_dev <- py$X_dev
    Y_dev <- py$Y_dev
    y_pred <- py$y_pred
    saveRDS(X_dev,X_dev_filename)
    saveRDS(Y_dev,Y_dev_filename)
    saveRDS(y_pred,Y_pred_filename)
  }
  saved_data <- list(X_dev,Y_dev,y_pred)
  names(saved_data) <- c('X_dev','Y_dev','y_pred')
  return(saved_data)
}
```

```{r}
save_results <- function(results,prose,ner_results,confusion_matrix,placenames,pic,dataset) {
saveRDS(results,paste0(data_dir,'/',dataset,"/",'results.rds'))
saveRDS(prose,paste0(data_dir,'/',dataset,"/",'prose.rds'))
saveRDS(ner_results,paste0(data_dir,'/',dataset,"/",'ner_results.rds'))
saveRDS(confusion_matrix,paste0(data_dir,'/',dataset,"/",'confusion_matrix.rds'))
saveRDS(placenames,paste0(data_dir,'/',dataset,"/",'placenames.rds'))
saveRDS(pic,paste0(data_dir,'/',dataset,"/",'confusion_plot.rds'))
}
```

```{python}
#wget https://data.deepai.org/conll2003.zip
def prep_python_env():
  data_dir = r.data_dir
  dataset = r.dataset
  model = r.model
  train_model = r.train_model
  print(train_model)
  from nltk.tag import pos_tag
  from sklearn.metrics import make_scorer,confusion_matrix
  from pprint import pprint
  from sklearn.metrics import f1_score,classification_report
  from sklearn.pipeline import Pipeline
  from sklearn_crfsuite import CRF, metrics
  import string
  import nltk
  #nltk.download('averaged_perceptron_tagger')
  X_train_file = data_dir + '/' + dataset + '/' + 'X_train.obj'
  Y_train_file = data_dir + '/' + dataset +  '/' + 'Y_train.obj'
  X_dev_file = data_dir + '/' + dataset + '/' + 'X_dev.obj'
  Y_dev_file = data_dir + '/' + dataset + '/' + 'Y_dev.obj'
  model_file = data_dir + '/' + model + '/' + 'crfmodel.obj'
  exists = os.path.isfile(X_dev_file)
  if exists and train_model:
    print("loading from disk")
    with open(X_train_file, 'rb') as X_train_f:
      X_train = pickle.load(X_train_f)
    with open(Y_train_file, 'rb') as Y_train_f:
      Y_train = pickle.load(Y_train_f)
    with open(X_dev_file, 'rb') as X_dev_f:
      X_dev = pickle.load(X_dev_f)
    with open(Y_dev_file, 'rb') as Y_dev_f:
      Y_dev = pickle.load(Y_dev_f)      
  elif exists:
    with open(X_dev_file, 'rb') as X_dev_f:
      X_dev = pickle.load(X_dev_f)
    with open(Y_dev_file, 'rb') as Y_dev_f:
      Y_dev = pickle.load(Y_dev_f)
    X_train = None
    Y_train = None
  elif train_model:
    X_train = r.X_train
    Y_train = r.Y_train
    Y_dev = r.Y_dev
    X_dev = r.X_dev
  else: 
    X_train = None
    Y_train = None
    Y_dev = r.Y_dev
    X_dev = r.X_dev
  with open(X_train_file, 'wb') as X_train_f:
    pickle.dump(X_train, X_train_f)
  with open(Y_train_file, 'wb') as Y_train_f:
    pickle.dump(Y_train, Y_train_f) 
  with open(X_dev_file, 'wb') as X_dev_f:
    pickle.dump(X_dev, X_dev_f) 
  with open(Y_dev_file, 'wb') as Y_dev_f:
    pickle.dump(Y_dev, Y_dev_f)
  return X_train, Y_train, X_dev, Y_dev, train_model, model_file
```







`

